{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14fd52a5-a9be-4f5e-b231-f35b0f491e03",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\tonyt\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: app-store-scraper in c:\\users\\tonyt\\anaconda3\\lib\\site-packages (0.3.5)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\tonyt\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\tonyt\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: flask in c:\\users\\tonyt\\anaconda3\\lib\\site-packages (3.0.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\tonyt\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\tonyt\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\tonyt\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\tonyt\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: requests==2.23.0 in c:\\users\\tonyt\\anaconda3\\lib\\site-packages (from app-store-scraper) (2.23.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\tonyt\\anaconda3\\lib\\site-packages (from requests==2.23.0->app-store-scraper) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\tonyt\\anaconda3\\lib\\site-packages (from requests==2.23.0->app-store-scraper) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\tonyt\\anaconda3\\lib\\site-packages (from requests==2.23.0->app-store-scraper) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tonyt\\anaconda3\\lib\\site-packages (from requests==2.23.0->app-store-scraper) (2024.8.30)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\tonyt\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\tonyt\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\tonyt\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: click in c:\\users\\tonyt\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\tonyt\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tonyt\\anaconda3\\lib\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: Werkzeug>=3.0.0 in c:\\users\\tonyt\\anaconda3\\lib\\site-packages (from flask) (3.0.3)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in c:\\users\\tonyt\\anaconda3\\lib\\site-packages (from flask) (3.1.4)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in c:\\users\\tonyt\\anaconda3\\lib\\site-packages (from flask) (2.2.0)\n",
      "Requirement already satisfied: blinker>=1.6.2 in c:\\users\\tonyt\\anaconda3\\lib\\site-packages (from flask) (1.6.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\tonyt\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tonyt\\anaconda3\\lib\\site-packages (from Jinja2>=3.1.2->flask) (2.1.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tonyt\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas app-store-scraper scikit-learn nltk flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55ff60f-a038-4dfe-ae07-08e6a8779234",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "317e87c3-96f7-4be3-a822-a36faaca167c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: urllib3 in c:\\users\\tonyt\\anaconda3\\lib\\site-packages (2.5.0)\n",
      "Requirement already satisfied: requests in c:\\users\\tonyt\\anaconda3\\lib\\site-packages (2.23.0)\n",
      "Collecting requests\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: six in c:\\users\\tonyt\\anaconda3\\lib\\site-packages (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\tonyt\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tonyt\\anaconda3\\lib\\site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tonyt\\anaconda3\\lib\\site-packages (from requests) (2024.8.30)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Installing collected packages: requests\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.23.0\n",
      "    Uninstalling requests-2.23.0:\n",
      "      Successfully uninstalled requests-2.23.0\n",
      "Successfully installed requests-2.32.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "app-store-scraper 0.3.5 requires requests==2.23.0, but you have requests 2.32.5 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade urllib3 requests six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "725522e5-c940-4bee-ac12-43e590362280",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: app-store-web-scraper in c:\\users\\tonyt\\anaconda3\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: urllib3<3,>=2.0.0 in c:\\users\\tonyt\\anaconda3\\lib\\site-packages (from app-store-web-scraper) (2.5.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tonyt\\AppData\\Local\\Temp\\ipykernel_8284\\2567098124.py:26: DeprecationWarning: 'AppReview.review' is deprecated, use 'content' instead.\n",
      "  'review': review.review\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped 500 reviews and saved to youcam_reviews.csv\n",
      "Here are the first 5 rows of your data:\n",
      "                       date  rating         userName  \\\n",
      "0 2025-09-20 08:10:15-07:00       1           jzy612   \n",
      "1 2025-09-19 18:36:43-07:00       5  Crea tus videos   \n",
      "2 2025-09-18 17:40:54-07:00       3           ùïìùï£ùïíùïüùïïùïö   \n",
      "3 2025-09-08 08:04:35-07:00       1   Mattysclerosis   \n",
      "4 2025-09-08 07:04:12-07:00       1        BREasy518   \n",
      "\n",
      "                                title  \\\n",
      "0                         Bad service   \n",
      "1                            La mejor   \n",
      "2                        Very glitchy   \n",
      "3                                Scam   \n",
      "4  Took payment before trial was over   \n",
      "\n",
      "                                              review  \n",
      "0  At first it was all good and when i try to sub...  \n",
      "1       Me encanta recomendada para editar tus fotos  \n",
      "2                Slow and glitchy. Frustrated to use  \n",
      "3  Said of you paid for the pro version you get 1...  \n",
      "4  I downloaded and looked at the app said a 7dat...  \n"
     ]
    }
   ],
   "source": [
    "# Step 1: Install the alternative scraper library\n",
    "!pip install app-store-web-scraper\n",
    "\n",
    "# Step 2: Import the necessary libraries\n",
    "from app_store_web_scraper import AppStoreEntry\n",
    "import pandas as pd\n",
    "\n",
    "# Step 3: Define the app details\n",
    "app_id = '768469908'\n",
    "country = 'us'\n",
    "\n",
    "# Step 4: Scrape the reviews using the new library\n",
    "app = AppStoreEntry(app_id=app_id, country=country)\n",
    "\n",
    "# First, create an empty list to store the reviews\n",
    "reviews_list = []\n",
    "\n",
    "# Then, loop through the reviews and add them to the list\n",
    "# Note: The underlying Apple API this library uses often limits results to a maximum of 500 reviews per app.\n",
    "for review in app.reviews(limit=2000):\n",
    "    reviews_list.append({\n",
    "        'date': review.date,\n",
    "        'rating': review.rating,\n",
    "        'userName': review.user_name,\n",
    "        'title': review.title,\n",
    "        'review': review.review\n",
    "    })\n",
    "\n",
    "# Step 5: Convert the list of reviews into a pandas DataFrame\n",
    "reviews_df = pd.DataFrame(reviews_list)\n",
    "\n",
    "# Step 6: Save the data to a CSV file and display the results\n",
    "if not reviews_df.empty:\n",
    "    reviews_df.to_csv('youcam_reviews.csv', index=False)\n",
    "    print(f\"Successfully scraped {len(reviews_df)} reviews and saved to youcam_reviews.csv\")\n",
    "    print(\"Here are the first 5 rows of your data:\")\n",
    "    print(reviews_df.head())\n",
    "else:\n",
    "    print(\"Failed to scrape any reviews. The app may not have reviews in this region, or the API is temporarily unavailable.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1e8244bc-9efd-4876-ad3b-0d118f2c5dac",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (471, 2)\n",
      "\n",
      "Sentiment distribution:\n",
      "sentiment\n",
      "0    264\n",
      "1    207\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Here are the first 5 rows of the prepared data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>At first it was all good and when i try to sub...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Me encanta recomendada para editar tus fotos</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Said of you paid for the pro version you get 1...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I downloaded and looked at the app said a 7dat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>This was a fantastic app, so much so AI purcha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  At first it was all good and when i try to sub...          0\n",
       "1       Me encanta recomendada para editar tus fotos          1\n",
       "3  Said of you paid for the pro version you get 1...          0\n",
       "4  I downloaded and looked at the app said a 7dat...          0\n",
       "5  This was a fantastic app, so much so AI purcha...          0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Load the CSV file into a pandas DataFrame\n",
    "import pandas as pd\n",
    "df = pd.read_csv('youcam_reviews.csv')\n",
    "\n",
    "# Step 2: Select only the columns we need for this project ('review' and 'rating')\n",
    "df = df[['review', 'rating']]\n",
    "\n",
    "# Step 3: Remove 3-star reviews, as they are often neutral and can confuse the model\n",
    "df = df[df['rating']!= 3]\n",
    "\n",
    "# Step 4: Create a new 'sentiment' column based on the rating.\n",
    "# If the rating is greater than 3 (i.e., 4 or 5), we label it as 1 (Positive).\n",
    "# Otherwise (i.e., 1 or 2), we label it as 0 (Negative).\n",
    "df['sentiment'] = df['rating'].apply(lambda rating: 1 if rating > 3 else 0)\n",
    "\n",
    "# Step 5: Drop the original 'rating' column as we no longer need it\n",
    "df = df.drop('rating', axis=1)\n",
    "\n",
    "# Step 6: Check the distribution of positive vs. negative reviews and display the first 5 rows\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nSentiment distribution:\")\n",
    "print(df['sentiment'].value_counts())\n",
    "print(\"\\nHere are the first 5 rows of the prepared data:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3ac8d9e3-228f-4010-9af4-4a3598671cbd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tonyt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tonyt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tonyt\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text preprocessing complete.\n",
      "Here's a comparison of the original vs. processed reviews:\n",
      "                                              review  \\\n",
      "0  At first it was all good and when i try to sub...   \n",
      "1       Me encanta recomendada para editar tus fotos   \n",
      "3  Said of you paid for the pro version you get 1...   \n",
      "4  I downloaded and looked at the app said a 7dat...   \n",
      "5  This was a fantastic app, so much so AI purcha...   \n",
      "\n",
      "                                    processed_review  \n",
      "0  first good try subscribe pick cheapest one did...  \n",
      "1          encanta recomendada para editar tus fotos  \n",
      "3  said paid pro version get free image video per...  \n",
      "4  downloaded looked app said dat trial use immed...  \n",
      "5  fantastic app much ai purchased vip plan year ...  \n"
     ]
    }
   ],
   "source": [
    "# Step 1: Download necessary NLTK data files\n",
    "# This is a one-time download. NLTK (Natural Language Toolkit) needs these files to understand stopwords,\n",
    "# how to tokenize sentences, and how to find the root of a word (lemmatization).\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Step 2: Create a function to clean the text\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Initialize the lemmatizer and stop words list\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Ensure the input is a string\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # 1. Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. Remove punctuation and numbers\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # 3. Tokenize the text into words\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # 4. Remove stop words and lemmatize\n",
    "    processed_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # 5. Join the words back into a single string\n",
    "    return \" \".join(processed_tokens)\n",
    "\n",
    "# Step 3: Apply the preprocessing function to the 'review' column\n",
    "# This creates a new column 'processed_review' with the cleaned text.\n",
    "df['processed_review'] = df['review'].apply(preprocess_text)\n",
    "\n",
    "# Step 4: Display the original review and the new processed review for comparison\n",
    "print(\"Text preprocessing complete.\")\n",
    "print(\"Here's a comparison of the original vs. processed reviews:\")\n",
    "print(df[['review', 'processed_review']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "79191de7-7e0f-4308-bc96-71e9b6056604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9158\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.89      0.96      0.93        53\n",
      "    Positive       0.95      0.86      0.90        42\n",
      "\n",
      "    accuracy                           0.92        95\n",
      "   macro avg       0.92      0.91      0.91        95\n",
      "weighted avg       0.92      0.92      0.92        95\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Step 1: Define your features (X) and target (y)\n",
    "# X is the processed text you want to use for prediction.\n",
    "# y is the sentiment label (0 or 1) you want to predict.\n",
    "X = df['processed_review']\n",
    "y = df['sentiment']\n",
    "\n",
    "# Step 2: Split the data into training and testing sets\n",
    "# We'll use 80% of the data for training and 20% for testing.\n",
    "# 'stratify=y' ensures that the proportion of positive and negative reviews is the same in both sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Step 3: Initialize and fit the TF-IDF Vectorizer\n",
    "# This will learn the vocabulary from your training data and convert the text into numerical vectors.\n",
    "# 'max_features=5000' limits the vocabulary to the 5000 most frequent words.\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "# Learn the vocabulary and transform the training data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Only transform the test data using the learned vocabulary\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Step 4: Train the Logistic Regression model\n",
    "lr_model = LogisticRegression(random_state=42)\n",
    "lr_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Step 5: Make predictions on the test set\n",
    "y_pred = lr_model.predict(X_test_tfidf)\n",
    "\n",
    "# Step 6: Evaluate the model's performance\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7f30c7df-fe2d-4c7e-89e3-45968a61a6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and vectorizer have been saved to files:\n",
      " - sentiment_model.pkl\n",
      " - tfidf_vectorizer.pkl\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Step 1: Save the trained Logistic Regression model\n",
    "joblib.dump(lr_model, 'sentiment_model.pkl')\n",
    "\n",
    "# Step 2: Save the TF-IDF Vectorizer\n",
    "joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.pkl')\n",
    "\n",
    "print(\"Model and vectorizer have been saved to files:\")\n",
    "print(\" - sentiment_model.pkl\")\n",
    "print(\" - tfidf_vectorizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9426b449-4fda-4387-a0c9-d695be6e9cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words for each identified user pain point (topic):\n",
      "Topic 1:\n",
      "app money thing ad tool pay lot away reason use\n",
      "Topic 2:\n",
      "free trial app charged refund year apple subscription day cancel\n",
      "Topic 3:\n",
      "app paying subscription pay photo make image premium need service\n",
      "Topic 4:\n",
      "app money time pay dont work use like waste month\n",
      "Topic 5:\n",
      "app feature photo use used pay im time free editing\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Step 1: Filter for only the negative reviews from your dataframe\n",
    "negative_reviews = df[df['sentiment'] == 0]['processed_review']\n",
    "\n",
    "# Step 2: Create a new vectorizer for topic modeling\n",
    "# LDA works better with simple word counts, so we use CountVectorizer instead of TF-IDF.\n",
    "count_vectorizer = CountVectorizer(max_df=0.9, min_df=5, stop_words='english')\n",
    "doc_term_matrix = count_vectorizer.fit_transform(negative_reviews)\n",
    "\n",
    "# Step 3: Train the LDA model to find 5 distinct topics\n",
    "# n_components is the number of topics you want to find. 5 is a good starting point.\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "lda.fit(doc_term_matrix)\n",
    "\n",
    "# Step 4: Display the top words for each discovered topic\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {topic_idx+1}:\")\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "print(\"Top words for each identified user pain point (topic):\")\n",
    "display_topics(lda, count_vectorizer.get_feature_names_out(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8cec2a36-e54e-4ddb-879a-60d8b8877a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved topic data to topics.csv\n",
      "          Topic  Strength                          Top Words\n",
      "0  Pain Point 1       303            app money thing ad tool\n",
      "1  Pain Point 2       918      free trial app charged refund\n",
      "2  Pain Point 3       405  app paying subscription pay photo\n",
      "3  Pain Point 4       727            app money time pay dont\n",
      "4  Pain Point 5      1050         app feature photo use used\n"
     ]
    }
   ],
   "source": [
    "# This function gets the top words for each topic\n",
    "def get_topics(model, feature_names, no_top_words):\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topic_name = f\"Pain Point {topic_idx+1}\"\n",
    "        topic_words = \" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]])\n",
    "        # For the chart, we'll use the model's internal topic strength as a placeholder value\n",
    "        topic_strength = int(topic.sum()) \n",
    "        topics.append([topic_name, topic_strength, topic_words])\n",
    "    return topics\n",
    "\n",
    "# Get the topics and their top words\n",
    "topics = get_topics(lda, count_vectorizer.get_feature_names_out(), 5)\n",
    "\n",
    "# Convert to a pandas DataFrame with the correct column names\n",
    "topics_df = pd.DataFrame(topics, columns=[\"Topic\", \"Strength\", \"Top Words\"])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "topics_df.to_csv('topics.csv', index=False)\n",
    "\n",
    "print(\"Successfully saved topic data to topics.csv\")\n",
    "print(topics_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a31eea-84b2-454e-89ba-3f6c7c528c1b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
